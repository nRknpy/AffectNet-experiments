{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchaffectnet.datasets import AffectNetDataset, AffectNetDatasetForSupCon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subDirectory_filePath</th>\n",
       "      <th>face_x</th>\n",
       "      <th>face_y</th>\n",
       "      <th>face_width</th>\n",
       "      <th>face_height</th>\n",
       "      <th>facial_landmarks</th>\n",
       "      <th>expression</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>689/737db2483489148d783ef278f43f486c0a97e140fc...</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>899</td>\n",
       "      <td>899</td>\n",
       "      <td>181.64;530.91;188.32;627.82;195.1;723.37;205.2...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>-0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392/c4db2f9b7e4b422d14b6e038f0cdc3ecee239b5532...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>137</td>\n",
       "      <td>137</td>\n",
       "      <td>28.82;77.52;29.12;93.25;31.04;108.51;33.03;123...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017253</td>\n",
       "      <td>0.004313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>468/21772b68dc8c2a11678c8739eca33adb6ccc658600...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>30.52;87.33;32.55;106.43;36.94;125.81;43.06;14...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.007937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>944/06e9ae8d3b240eb68fa60534783eacafce2def60a8...</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>269</td>\n",
       "      <td>269</td>\n",
       "      <td>44.43;158.17;47.08;189.2;50.54;221.88;58.3;253...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153401</td>\n",
       "      <td>0.038890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>993/02e06ee5521958b4042dd73abb444220609d96f57b...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>50.59;78.72;48.6;93.23;48.72;109.06;48.8;123.0...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.783972</td>\n",
       "      <td>-0.551684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414794</th>\n",
       "      <td>1215/65e2c4fe91780f417c0edf3c71561d0e41e4db248...</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>83.9;225.92;88.93;260.62;97.28;296.78;108.34;3...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.662960</td>\n",
       "      <td>-0.249501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414795</th>\n",
       "      <td>375/a0077b9ae7ab3fd9241dd0775cb9f88c1d8913c7af...</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>330</td>\n",
       "      <td>330</td>\n",
       "      <td>106.87;161.43;105.49;191.66;109.81;224.48;113....</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>-0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414796</th>\n",
       "      <td>606/08e4e677c377461ec5400d74b4fd07dd6454cf2d7e...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "      <td>52.53;97.98;50.5;116.98;50.65;136.12;53.3;155....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542562</td>\n",
       "      <td>-0.015803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414797</th>\n",
       "      <td>867/d5c098b92c063caf1c8761c9b4ee5d7a8c3c08f94b...</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>724</td>\n",
       "      <td>724</td>\n",
       "      <td>95.85;340.5;99.96;432.63;117.87;521.21;138.52;...</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414798</th>\n",
       "      <td>1372/115ea14e515a51da3b4aa021ac6b927296566de0d...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>266</td>\n",
       "      <td>266</td>\n",
       "      <td>9.41;132.61;11.55;165.9;16.94;200.19;27.08;232...</td>\n",
       "      <td>10</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414799 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    subDirectory_filePath  ...   arousal\n",
       "0       689/737db2483489148d783ef278f43f486c0a97e140fc...  ... -0.055556\n",
       "1       392/c4db2f9b7e4b422d14b6e038f0cdc3ecee239b5532...  ...  0.004313\n",
       "2       468/21772b68dc8c2a11678c8739eca33adb6ccc658600...  ...  0.007937\n",
       "3       944/06e9ae8d3b240eb68fa60534783eacafce2def60a8...  ...  0.038890\n",
       "4       993/02e06ee5521958b4042dd73abb444220609d96f57b...  ... -0.551684\n",
       "...                                                   ...  ...       ...\n",
       "414794  1215/65e2c4fe91780f417c0edf3c71561d0e41e4db248...  ... -0.249501\n",
       "414795  375/a0077b9ae7ab3fd9241dd0775cb9f88c1d8913c7af...  ... -0.380952\n",
       "414796  606/08e4e677c377461ec5400d74b4fd07dd6454cf2d7e...  ... -0.015803\n",
       "414797  867/d5c098b92c063caf1c8761c9b4ee5d7a8c3c08f94b...  ... -2.000000\n",
       "414798  1372/115ea14e515a51da3b4aa021ac6b927296566de0d...  ... -2.000000\n",
       "\n",
       "[414799 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../Affectnet/training.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         181.64;530.91;188.32;627.82;195.1;723.37;205.2...\n",
       "1         28.82;77.52;29.12;93.25;31.04;108.51;33.03;123...\n",
       "2         30.52;87.33;32.55;106.43;36.94;125.81;43.06;14...\n",
       "3         44.43;158.17;47.08;189.2;50.54;221.88;58.3;253...\n",
       "4         50.59;78.72;48.6;93.23;48.72;109.06;48.8;123.0...\n",
       "                                ...                        \n",
       "414794    83.9;225.92;88.93;260.62;97.28;296.78;108.34;3...\n",
       "414795    106.87;161.43;105.49;191.66;109.81;224.48;113....\n",
       "414796    52.53;97.98;50.5;116.98;50.65;136.12;53.3;155....\n",
       "414797    95.85;340.5;99.96;432.63;117.87;521.21;138.52;...\n",
       "414798    9.41;132.61;11.55;165.9;16.94;200.19;27.08;232...\n",
       "Name: facial_landmarks, Length: 414799, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['facial_landmarks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [181.64, 530.91, 188.32, 627.82, 195.1, 723.37...\n",
       "1         [28.82, 77.52, 29.12, 93.25, 31.04, 108.51, 33...\n",
       "2         [30.52, 87.33, 32.55, 106.43, 36.94, 125.81, 4...\n",
       "3         [44.43, 158.17, 47.08, 189.2, 50.54, 221.88, 5...\n",
       "4         [50.59, 78.72, 48.6, 93.23, 48.72, 109.06, 48....\n",
       "                                ...                        \n",
       "414794    [83.9, 225.92, 88.93, 260.62, 97.28, 296.78, 1...\n",
       "414795    [106.87, 161.43, 105.49, 191.66, 109.81, 224.4...\n",
       "414796    [52.53, 97.98, 50.5, 116.98, 50.65, 136.12, 53...\n",
       "414797    [95.85, 340.5, 99.96, 432.63, 117.87, 521.21, ...\n",
       "414798    [9.41, 132.61, 11.55, 165.9, 16.94, 200.19, 27...\n",
       "Name: facial_landmarks, Length: 414799, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['facial_landmarks'].str.split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_float = df['facial_landmarks'].str.split(';').apply(lambda x: [float(val) for val in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [181.64, 530.91, 188.32, 627.82, 195.1, 723.37...\n",
       "1         [28.82, 77.52, 29.12, 93.25, 31.04, 108.51, 33...\n",
       "2         [30.52, 87.33, 32.55, 106.43, 36.94, 125.81, 4...\n",
       "3         [44.43, 158.17, 47.08, 189.2, 50.54, 221.88, 5...\n",
       "4         [50.59, 78.72, 48.6, 93.23, 48.72, 109.06, 48....\n",
       "                                ...                        \n",
       "414794    [83.9, 225.92, 88.93, 260.62, 97.28, 296.78, 1...\n",
       "414795    [106.87, 161.43, 105.49, 191.66, 109.81, 224.4...\n",
       "414796    [52.53, 97.98, 50.5, 116.98, 50.65, 136.12, 53...\n",
       "414797    [95.85, 340.5, 99.96, 432.63, 117.87, 521.21, ...\n",
       "414798    [9.41, 132.61, 11.55, 165.9, 16.94, 200.19, 27...\n",
       "Name: facial_landmarks, Length: 414799, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "landmarks_tensor = torch.tensor(landmarks_float.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([414799, 136])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m     y_coordinates\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Convert the lists of coordinates into torch.tensors\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m x_landmarks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(x_coordinates)\n\u001b[1;32m     22\u001b[0m y_landmarks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_coordinates)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "landmarks = df['facial_landmarks']\n",
    "face_x = df['face_x']\n",
    "face_y = df['face_y']\n",
    "face_width = df['face_width']\n",
    "face_height = df['face_height']\n",
    "# Split the string values by semicolon (;)\n",
    "landmarks_split = landmarks.str.split(';')\n",
    "# Initialize lists for x and y coordinates\n",
    "x_coordinates = []\n",
    "y_coordinates = []\n",
    "# Iterate through each split value and separate x and y coordinates\n",
    "for landmark, x_min, y_min, w, h in zip(landmarks_split, face_x, face_y, face_width, face_height):\n",
    "    coordinates = [float(val) for val in landmark]\n",
    "    x = torch.tensor(coordinates[::2])\n",
    "    y = torch.tensor(coordinates[1::2])\n",
    "    x = (x - x_min)/w\n",
    "    y = (y - y_min)/h\n",
    "    x_coordinates.append(x)\n",
    "    y_coordinates.append(y)\n",
    "# Convert the lists of coordinates into torch.tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_landmarks = torch.stack(x_coordinates)\n",
    "y_landmarks = torch.stack(y_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.2992e-02,  6.0423e-02,  6.7964e-02,  ...,  5.5151e-01,\n",
       "          5.0744e-01,  4.6313e-01],\n",
       "        [ 6.4380e-02,  6.6569e-02,  8.0584e-02,  ...,  5.4628e-01,\n",
       "          5.0088e-01,  4.6197e-01],\n",
       "        [ 1.1091e-01,  1.2244e-01,  1.4739e-01,  ...,  5.5852e-01,\n",
       "          5.1614e-01,  4.8114e-01],\n",
       "        ...,\n",
       "        [ 1.2843e-01,  1.1780e-01,  1.1859e-01,  ...,  5.1089e-01,\n",
       "          4.5576e-01,  4.1272e-01],\n",
       "        [ 4.2610e-02,  4.8287e-02,  7.3025e-02,  ...,  5.5059e-01,\n",
       "          4.9775e-01,  4.5076e-01],\n",
       "        [-2.8534e-02, -2.0489e-02, -2.2556e-04,  ...,  6.2135e-01,\n",
       "          5.7346e-01,  5.2699e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([414799, 68])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class AffectNetDatasetForSupConWithLandmark(AffectNetDatasetForSupCon):\n",
    "    def __init__(self, csvfile: str, root: str, transform1, transform2, exclude_label: Tuple[int] = ..., return_labels: bool = True, crop: bool = False, invalid_files: List[str] = None):\n",
    "        super().__init__(csvfile, root, transform1, transform2, exclude_label, return_labels, crop, invalid_files)\n",
    "        landmarks = self.df['facial_landmarks']\n",
    "        face_x = self.df['face_x']\n",
    "        face_y = self.df['face_y']\n",
    "        face_width = self.df['face_width']\n",
    "        face_height = self.df['face_height']\n",
    "        landmarks_split = landmarks.str.split(';')\n",
    "        x_coordinates = []\n",
    "        y_coordinates = []\n",
    "        for landmark, x_min, y_min, w, h in zip(landmarks_split, face_x, face_y, face_width, face_height):\n",
    "            coordinates = [float(val) for val in landmark]\n",
    "            x = torch.tensor(coordinates[::2])\n",
    "            y = torch.tensor(coordinates[1::2])\n",
    "            x = (x - x_min)/w\n",
    "            y = (y - y_min)/h\n",
    "            x_coordinates.append(x)\n",
    "            y_coordinates.append(y)\n",
    "        self.x_landmarks = torch.stack(x_coordinates)\n",
    "        self.y_landmarks = torch.stack(y_coordinates)\n",
    "    \n",
    "    def labeling(self, idx):\n",
    "        x_landmark = self.x_landmarks[idx]\n",
    "        y_landmark = self.y_landmarks[idx]\n",
    "        return torch.stack([x_landmark, y_landmark])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkn/.local/share/virtualenvs/AffectNet-experiments-e17pgDV_/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                  num_labels=64,\n",
    "                                                  problem_type='regression',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (Compose,\n",
    "                                    Normalize,\n",
    "                                    Resize,\n",
    "                                    RandomResizedCrop,\n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomApply,\n",
    "                                    ColorJitter,\n",
    "                                    RandomGrayscale,\n",
    "                                    ToTensor,\n",
    "                                    RandomAffine)\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean,\n",
    "                          std=feature_extractor.image_std)\n",
    "\n",
    "transform1 = Compose([\n",
    "        RandomAffine(30),\n",
    "        Resize(tuple(feature_extractor.size.values())),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "transform2 = Compose([\n",
    "    RandomResizedCrop(size=tuple(\n",
    "        feature_extractor.size.values()), scale=(0.2, 1.)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomApply([\n",
    "        ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    ], p=0.8),\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "dataset = AffectNetDatasetForSupConWithLandmark('../../Affectnet/training.csv',\n",
    "                                                '../../Affectnet/Manually_Annotated/Manually_Annotated_Images/',\n",
    "                                                transform1,\n",
    "                                                transform2,\n",
    "                                                [8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "  \n",
       "          [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "  \n",
       "          [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           ...,\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "           [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
       "  tensor([[[-0.7725, -0.8275, -0.8431,  ..., -0.7176, -0.7255, -0.7176],\n",
       "           [-0.8902, -0.7725, -0.8118,  ..., -0.7333, -0.7490, -0.7647],\n",
       "           [-0.6549, -0.7176, -0.7490,  ..., -0.7490, -0.7333, -0.7490],\n",
       "           ...,\n",
       "           [-0.6314, -0.6314, -0.5765,  ..., -0.3098, -0.2627, -0.2627],\n",
       "           [-0.6235, -0.6549, -0.5922,  ..., -0.4275, -0.3255, -0.3020],\n",
       "           [-0.5922, -0.6549, -0.5922,  ..., -0.5529, -0.4824, -0.3882]],\n",
       "  \n",
       "          [[-0.8039, -0.8510, -0.8745,  ..., -0.7020, -0.7098, -0.7176],\n",
       "           [-0.8902, -0.8039, -0.8431,  ..., -0.7333, -0.7412, -0.7490],\n",
       "           [-0.6863, -0.7490, -0.7804,  ..., -0.7412, -0.7333, -0.7412],\n",
       "           ...,\n",
       "           [-0.6157, -0.6157, -0.5686,  ..., -0.3255, -0.2784, -0.2784],\n",
       "           [-0.6235, -0.6549, -0.5922,  ..., -0.4353, -0.3333, -0.3098],\n",
       "           [-0.5843, -0.6392, -0.5922,  ..., -0.5765, -0.4902, -0.4039]],\n",
       "  \n",
       "          [[-0.7961, -0.8431, -0.8745,  ..., -0.7490, -0.7569, -0.7569],\n",
       "           [-0.8980, -0.7961, -0.8431,  ..., -0.7569, -0.7725, -0.7882],\n",
       "           [-0.6863, -0.7490, -0.7804,  ..., -0.7725, -0.7569, -0.7725],\n",
       "           ...,\n",
       "           [-0.6627, -0.6627, -0.6235,  ..., -0.4118, -0.3490, -0.3490],\n",
       "           [-0.6627, -0.6941, -0.6235,  ..., -0.5137, -0.4196, -0.3882],\n",
       "           [-0.6157, -0.6863, -0.6235,  ..., -0.6392, -0.5765, -0.4745]]])),\n",
       " tensor([[0.0764, 0.0821, 0.1045, 0.1322, 0.1756, 0.2373, 0.3187, 0.4154, 0.5309,\n",
       "          0.6242, 0.6975, 0.7591, 0.8131, 0.8512, 0.8740, 0.8858, 0.8845, 0.1670,\n",
       "          0.2324, 0.3093, 0.3867, 0.4645, 0.5653, 0.6365, 0.7073, 0.7790, 0.8404,\n",
       "          0.5242, 0.5276, 0.5314, 0.5348, 0.4336, 0.4836, 0.5342, 0.5838, 0.6200,\n",
       "          0.2477, 0.2958, 0.3500, 0.3976, 0.3468, 0.2956, 0.6262, 0.6738, 0.7252,\n",
       "          0.7637, 0.7288, 0.6808, 0.3529, 0.4208, 0.4776, 0.5275, 0.5789, 0.6305,\n",
       "          0.6928, 0.6395, 0.5854, 0.5311, 0.4808, 0.4215, 0.3737, 0.4778, 0.5270,\n",
       "          0.5799, 0.6729, 0.5824, 0.5288, 0.4787],\n",
       "         [0.3687, 0.4816, 0.5966, 0.7146, 0.8184, 0.9253, 1.0287, 1.1091, 1.1189,\n",
       "          1.1042, 1.0302, 0.9353, 0.8247, 0.7133, 0.6004, 0.4824, 0.3609, 0.3588,\n",
       "          0.3254, 0.3288, 0.3421, 0.3660, 0.3688, 0.3346, 0.3108, 0.2987, 0.3238,\n",
       "          0.4155, 0.5102, 0.6036, 0.6976, 0.7090, 0.7293, 0.7476, 0.7261, 0.7019,\n",
       "          0.4069, 0.3865, 0.3876, 0.4175, 0.4285, 0.4288, 0.4043, 0.3751, 0.3724,\n",
       "          0.3860, 0.4073, 0.4115, 0.8408, 0.8331, 0.8289, 0.8446, 0.8269, 0.8242,\n",
       "          0.8291, 0.8855, 0.9119, 0.9205, 0.9147, 0.8910, 0.8458, 0.8538, 0.8632,\n",
       "          0.8512, 0.8342, 0.8637, 0.8749, 0.8659]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AffectNet-experiments-e17pgDV_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
